#!/usr/bin/env python3

# Weka Prometheus client
# Vince Fleming
# vince@weka.io
#
VERSION="0.9.5"

# system imports
import logging
import logging.handlers
from logging import debug, info, warning, error, critical
import argparse
import os
import sys
from prometheus_client import start_http_server, REGISTRY
import time
import traceback
from lokilogs import LokiServer
from multiprocessing import Process

# local imports
from collector import wekaCollector
from wekacluster import WekaCluster
import signals


def prom_client(port, configfile, cluster_hosts, cluster_auth, lokihost, lokiport):

    try:
        cluster_obj = WekaCluster(cluster_hosts, cluster_auth)
    except Exception as exc:
        track = traceback.format_exc()
        print(track)
        logger.critical(f"unable to create cluster object: {exc}")
        sys.exit(1)

    # create the wekaCollector object
    collector = wekaCollector(args.configfile, cluster_obj)

    if lokihost != None:
        lokiserver = LokiServer( lokihost, lokiport )
    else:
        lokiserver = None

    #
    # Start up the server to expose the metrics.
    #
    logger.info(f"starting http server on port {port}" )
    start_http_server(int(port))

    # register our custom collector
    REGISTRY.register( collector )

    while True:
        time.sleep(30)  # sleep first, just in case we're started at the same time as Loki; give it time
        if lokiserver != None:
            logger.info(f"getting events for cluster {cluster_obj.name}")
            try:
                events = cluster_obj.get_events()
                lokiserver.send_events(events, cluster_obj)
            except Exception as exc:
                logger.critical(f"Error getting events: {exc} for cluster {cluster_obj.name}")



if __name__ == '__main__':
    # handle signals (ie: ^C and such)
    signals.signal_handling()

    parser = argparse.ArgumentParser(description="Prometheus Client for Weka clusters")
    parser.add_argument("-c", "--configfile", dest='configfile', default="./export.yml", help="override ./export.yml as config file")
    parser.add_argument("-p", "--port", dest='port', default="8001", help="TCP port number to listen on")
    parser.add_argument("--loki_host", dest='lokihost', default=None, help="hostname/ip for loki server")
    parser.add_argument("--loki_port", dest='lokiport', default="3100", help="port for loki server")
    parser.add_argument( 'clusterspec', default="localhost", nargs='*', help="Cluster specifications.  <host>,<host>,...:authfile" )
    #parser.add_argument("-a", "--autohost", dest='autohost', default=False, action="store_true", help="Automatically load balance queries over backend hosts" )
    parser.add_argument("-v", "--verbosity", action="count", default=0, help="increase output verbosity")
    parser.add_argument("--version", dest="version", default=False, action="store_true", help="Display version number")
    args = parser.parse_args()

    if args.version:
        print( f"{sys.argv[0]} version {VERSION}" )
        sys.exit( 0 )

    if args.verbosity == 0:
        loglevel = logging.ERROR
    elif args.verbosity == 1:
        loglevel = logging.WARNING
    elif args.verbosity == 2:
        loglevel = logging.INFO
    elif args.verbosity > 2:
        loglevel = logging.DEBUG

    # set the root logger
    logger = logging.getLogger()
    FORMAT = "%(process)s:%(filename)s:%(lineno)s:%(funcName)s():%(message)s"
    #logging.basicConfig(format=FORMAT)
    logger.setLevel(loglevel)

    # create handler to log to syslog
    syslog_handler = logging.handlers.SysLogHandler(address="/dev/log")
    #syslog_handler.setLevel(loglevel)
    syslog_handler.setFormatter(logging.Formatter(os.path.basename( sys.argv[0] ) + ': %(levelname)s: %(message)s'))

    # create handler to log to stderr
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(FORMAT))
    #console_handler.setLevel(loglevel)

    # add handlers to root logger
    #logger.addHandler(syslog_handler)
    logger.addHandler(console_handler)

    # configure logging in signals module
    #sublog = logging.getLogger( "signals" )
    #sublog.setLevel(loglevel)

    # configure logging in collector module
    sublog = logging.getLogger( "collector" )
    sublog.setLevel(logging.INFO)

    # configure logging in wekaapi module
    sublog = logging.getLogger( "wekaapi" )
    sublog.setLevel(logging.INFO)

    # configure logging in sthreads module
    sublog = logging.getLogger( "sthreads" )
    sublog.setLevel(logging.ERROR)

    # configure logging in lokilogs module
    sublog = logging.getLogger( "lokilogs" )
    sublog.setLevel(logging.INFO)

    # configure logging in circular module
    logging.getLogger("circular").setLevel(logging.INFO)

    # schedule up a process for each cluster, put them on conescutive ports starting at 8001 (or specified port)
    subprocesses = {}
    port = int(args.port)

    for spec in args.clusterspec:
        clusterspeclist = spec.split(":")
        #cluster_hosts = clusterspeclist[0].split(',')
        if len(clusterspeclist) > 1:
            cluster_auth = clusterspeclist[1]
        else:
            cluster_auth = None
        p = Process(target=prom_client, 
                args=(port, args.configfile, spec, cluster_auth, args.lokihost, args.lokiport))
        subprocesses[spec] = p  # keep processes by clusterspec so easy to tell them apart
        port += 1
        
    # kick them off
    for clusterspec, proc in subprocesses.items():
        logger.info(f"starting processing of cluster {clusterspec}")
        proc.start()

    while True:
        time.sleep(10)
        # monitor subprocesses
        for clusterspec, proc in subprocesses.items():
            if not proc.is_alive():
                logger.critical(f"Child process for cluster {clusterspec} died.")
                proc.join()
                # do we try to restart it?

